{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import torch\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tags = [], []\n",
    "\n",
    "with open(r'C:\\Users\\anu10961\\Work\\POC\\submission\\autonomiq\\data\\tokens.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        if len(row): tokens.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\anu10961\\Work\\POC\\submission\\autonomiq\\data\\tags.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader)\n",
    "    for row in csv_reader:\n",
    "        if len(row): tags.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 434)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity(tokens, tags, diff=0):\n",
    "    for l1, l2 in zip(tokens, tags):\n",
    "        if abs(len(l1) - len(l2)) != diff: raise Exception\n",
    "            \n",
    "sanity(tokens, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, valid_tokens, tr_tags, val_tags = train_test_split(tokens, tags, test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = [\"action\", \"label\", \"data\", \"O\"]\n",
    "\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'action': 0, 'label': 1, 'data': 2, 'O': 3},\n",
       " {0: 'action', 1: 'label', 2: 'data', 3: 'O'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2id, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_token_tags(tokens_list, tags_list):\n",
    "    tokenized_ids, tokenized_tags = [], []\n",
    "    for tokens, tags in zip(tokens_list, tags_list):\n",
    "        target_ids, target_tags = list(), list()\n",
    "        for index, token in enumerate(tokens):\n",
    "            token_breakup_list = tokenizer.encode(token, add_special_tokens=False)\n",
    "            num_items = len(token_breakup_list)\n",
    "            target_ids.extend(token_breakup_list)\n",
    "            tag = tags[index]\n",
    "            target_tags.extend([tag] * num_items)\n",
    "        tokenized_ids.append(target_ids)\n",
    "        tokenized_tags.append([\"O\"] + target_tags + [\"O\"])\n",
    "        \n",
    "    return tokenized_ids, tokenized_tags\n",
    "\n",
    "train_ids, train_tags = get_tokenized_token_tags(train_tokens, tr_tags)\n",
    "valid_ids, valid_tags = get_tokenized_token_tags(valid_tokens, val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \" \".join(['navigate', 'to', 'payables', '-', '>', 'payments', 'and', 'deselect', 'manage', 'payments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "navigate to payables - > payments and deselect manage payments\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(str1)\n",
    "print(len(str1.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pay', '##able', '##s']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ids = tokenizer(\"payables\").get(\"input_ids\")\n",
    "\n",
    "tokenizer.convert_ids_to_tokens(tok_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['navigate', 'to', 'pay', '##able', '##s', '-', '>', 'payments', 'and', 'des', '##ele', '##ct', 'manage', 'payments']\n",
      "14\n",
      "['O', 'action', 'O', 'label', 'label', 'label', 'O', 'O', 'label', 'O', 'action', 'action', 'action', 'label', 'label', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(train_ids[0]))\n",
    "print(len(tokenizer.convert_ids_to_tokens(train_ids[0])))\n",
    "print(train_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity(train_ids, train_tags, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity(valid_ids, valid_tags, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_tokens, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)\n",
    "val_encodings = tokenizer(valid_tokens, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=110, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_encodings[0].attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_train = max([len(x) for x in train_encodings[\"input_ids\"]])\n",
    "max_len_val = max([len(x) for x in val_encodings[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 67)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_train, max_len_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tags(tags, max_len):\n",
    "    #labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels, otag_id = [], tag2id[\"O\"]\n",
    "    for doc_labels in tags:\n",
    "        tmp_list = [-100 for i in range(max_len)]\n",
    "        for index, item in enumerate(doc_labels):\n",
    "            tmp_list[index] = tag2id[item]\n",
    "        encoded_labels.append(tmp_list)\n",
    "    return encoded_labels\n",
    "\n",
    "train_labels = encode_tags(train_tags, max_len_train)\n",
    "val_labels = encode_tags(valid_tags, max_len_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AutonomIQDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_encodings.pop(\"offset_mapping\") # we don't want to pass this to the model\n",
    "val_encodings.pop(\"offset_mapping\")\n",
    "\n",
    "train_dataset = AutonomIQDataset(train_encodings, train_labels)\n",
    "val_dataset = AutonomIQDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForTokenClassification\n",
    "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(unique_tags))\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "average = \"micro\"\n",
    "\n",
    "def compute_metrics(p):\n",
    "    average = \"micro\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        p for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100 \n",
    "    ]\n",
    "    true_labels = [\n",
    "        l for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100\n",
    "    ]\n",
    "\n",
    "    #results = classification_report(true_labels, true_predictions, target_names=tag2id, output_dict=True)\n",
    "    results = {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions, average=average),\n",
    "        \"recall\": recall_score(true_labels, true_predictions, average=average),\n",
    "        \"f1\": f1_score(true_labels, true_predictions, average=average)\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_per_label(labels, predictions):\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        p for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100 \n",
    "    ]\n",
    "    true_labels = [\n",
    "        l for prediction, label in zip(predictions, labels) for (p, l) in zip(prediction, label) if l != -100\n",
    "    ]\n",
    "    \n",
    "    print(classification_report(true_labels, true_predictions, target_names=tag2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    y_true, y_pred = [], []\n",
    "    for item in dataset:\n",
    "        val_labels, input_ids, attention_mask = item.get(\"labels\").tolist(), item.get(\"input_ids\"), item.get(\"attention_mask\")\n",
    "        input_ids, attention_mask = input_ids.resize(1, input_ids.size()[0]).cuda(), attention_mask.resize(1, input_ids.size()[0]).cuda()\n",
    "        output = model.forward(input_ids, attention_mask)\n",
    "        tokens = input_ids.tolist()\n",
    "        pred_labels = output[0].argmax(2).tolist()\n",
    "        \n",
    "        y_true.append(val_labels)\n",
    "        y_pred.append(pred_labels[0])\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\arcgis183\\lib\\site-packages\\torch\\tensor.py:362: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      action       0.17      0.69      0.27       125\n",
      "       label       0.21      0.10      0.13       191\n",
      "        data       0.13      0.28      0.17        25\n",
      "           O       0.47      0.14      0.22       456\n",
      "\n",
      "    accuracy                           0.22       797\n",
      "   macro avg       0.24      0.30      0.20       797\n",
      "weighted avg       0.35      0.22      0.21       797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = evaluate(val_dataset)\n",
    "compute_metrics_per_label(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\arcgis183\\lib\\site-packages\\transformers\\training_args.py:299: FutureWarning: The `evaluate_during_training` argument is deprecated in favor of `evaluation_strategy` (which has more options)\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705ec7c48ce64fb1a8e7de3630063314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e500b9839e43928df3bc7d10940ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3458271026611328, 'learning_rate': 1.25e-05, 'epoch': 0.4, 'total_flos': 7008244953600, 'step': 10}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462146380cb7426c9e562d45f965a463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 1.2045083045959473, 'eval_accuracy': 0.5784190715181933, 'eval_precision': 0.5784190715181933, 'eval_recall': 0.5784190715181933, 'eval_f1': 0.5784190715181933, 'epoch': 0.4, 'total_flos': 7008244953600, 'step': 10}\n",
      "{'loss': 1.0510946273803712, 'learning_rate': 2.5e-05, 'epoch': 0.8, 'total_flos': 14016489907200, 'step': 20}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d708344ea94f3482a2043da0635f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.8882801532745361, 'eval_accuracy': 0.5759096612296111, 'eval_precision': 0.5759096612296111, 'eval_recall': 0.5759096612296111, 'eval_f1': 0.5759096612296111, 'epoch': 0.8, 'total_flos': 14016489907200, 'step': 20}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a309f4b2a7fc4a6882f9296b85a5ff99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7429004669189453, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.2, 'total_flos': 20586719551200, 'step': 30}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e29d5fb00b24c2a870c16bee982372e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.5230689644813538, 'eval_accuracy': 0.823086574654956, 'eval_precision': 0.823086574654956, 'eval_recall': 0.823086574654956, 'eval_f1': 0.823086574654956, 'epoch': 1.2, 'total_flos': 20586719551200, 'step': 30}\n",
      "{'loss': 0.4461969375610352, 'learning_rate': 5e-05, 'epoch': 1.6, 'total_flos': 27594964504800, 'step': 40}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37330f44fb9849758788481d1f78ea25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.3081311285495758, 'eval_accuracy': 0.8820577164366374, 'eval_precision': 0.8820577164366374, 'eval_recall': 0.8820577164366374, 'eval_f1': 0.8820577164366374, 'epoch': 1.6, 'total_flos': 27594964504800, 'step': 40}\n",
      "{'loss': 0.2697273254394531, 'learning_rate': 4.411764705882353e-05, 'epoch': 2.0, 'total_flos': 34165194148800, 'step': 50}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55336f9532242af820ff3ebb7ccc2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.22805634140968323, 'eval_accuracy': 0.917189460476788, 'eval_precision': 0.917189460476788, 'eval_recall': 0.917189460476788, 'eval_f1': 0.917189460476788, 'epoch': 2.0, 'total_flos': 34165194148800, 'step': 50}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b90e89d4bad41e9ae6e88f26bcb880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.20343360900878907, 'learning_rate': 3.8235294117647055e-05, 'epoch': 2.4, 'total_flos': 41173439102400, 'step': 60}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23930dc553d6450c81515b934262d7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.22505517303943634, 'eval_accuracy': 0.9360100376411543, 'eval_precision': 0.9360100376411543, 'eval_recall': 0.9360100376411543, 'eval_f1': 0.9360100376411543, 'epoch': 2.4, 'total_flos': 41173439102400, 'step': 60}\n",
      "{'loss': 0.13460731506347656, 'learning_rate': 3.235294117647059e-05, 'epoch': 2.8, 'total_flos': 48181684056000, 'step': 70}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6ccb08ce6a4880b57d8e8944dc49de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.19071367383003235, 'eval_accuracy': 0.9435382685069009, 'eval_precision': 0.9435382685069009, 'eval_recall': 0.9435382685069009, 'eval_f1': 0.9435382685069009, 'epoch': 2.8, 'total_flos': 48181684056000, 'step': 70}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1173e08103414db1e370400e2a2a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.10393409729003907, 'learning_rate': 2.647058823529412e-05, 'epoch': 3.2, 'total_flos': 54751913700000, 'step': 80}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdd2876eb0649ad9bbe954084318a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.18758872151374817, 'eval_accuracy': 0.9473023839397742, 'eval_precision': 0.9473023839397742, 'eval_recall': 0.9473023839397742, 'eval_f1': 0.9473023839397742, 'epoch': 3.2, 'total_flos': 54751913700000, 'step': 80}\n",
      "{'loss': 0.08570594787597656, 'learning_rate': 2.058823529411765e-05, 'epoch': 3.6, 'total_flos': 61760158653600, 'step': 90}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06ffc59532341aa95bf4aa55811a091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.18657055497169495, 'eval_accuracy': 0.9523212045169385, 'eval_precision': 0.9523212045169385, 'eval_recall': 0.9523212045169385, 'eval_f1': 0.9523212045169385, 'epoch': 3.6, 'total_flos': 61760158653600, 'step': 90}\n",
      "{'loss': 0.07507057189941406, 'learning_rate': 1.4705882352941177e-05, 'epoch': 4.0, 'total_flos': 68330388297600, 'step': 100}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1429c24e9a944880aca67f58906b8d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.17002694308757782, 'eval_accuracy': 0.9510664993726474, 'eval_precision': 0.9510664993726474, 'eval_recall': 0.9510664993726474, 'eval_f1': 0.9510664993726474, 'epoch': 4.0, 'total_flos': 68330388297600, 'step': 100}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b776251a6cb44d9a8c6805d6e1db3fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.04829330444335937, 'learning_rate': 8.823529411764707e-06, 'epoch': 4.4, 'total_flos': 75338633251200, 'step': 110}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597b1f75b78f4557894122ce97b201c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.16724401712417603, 'eval_accuracy': 0.958594730238394, 'eval_precision': 0.958594730238394, 'eval_recall': 0.958594730238394, 'eval_f1': 0.958594730238394, 'epoch': 4.4, 'total_flos': 75338633251200, 'step': 110}\n",
      "{'loss': 0.050584030151367185, 'learning_rate': 2.9411764705882355e-06, 'epoch': 4.8, 'total_flos': 82346878204800, 'step': 120}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988c5d1264fd4ed88acb9efe840386ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Evaluation'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eval_loss': 0.16824127733707428, 'eval_accuracy': 0.958594730238394, 'eval_precision': 0.958594730238394, 'eval_recall': 0.958594730238394, 'eval_f1': 0.958594730238394, 'epoch': 4.8, 'total_flos': 82346878204800, 'step': 120}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.3667749328613281)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag2id.pop(\"O\", None)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=40,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    do_eval=True,\n",
    "    evaluate_during_training=True,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      action       0.98      0.98      0.98       125\n",
      "       label       0.92      0.92      0.92       191\n",
      "        data       1.00      0.96      0.98        25\n",
      "           O       0.96      0.96      0.96       456\n",
      "\n",
      "    accuracy                           0.96       797\n",
      "   macro avg       0.97      0.96      0.96       797\n",
      "weighted avg       0.96      0.96      0.96       797\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = evaluate(val_dataset)\n",
    "compute_metrics_per_label(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    text = re.sub('([!?,\\'\".\\n\\-\\:\\*\\/])', r' \\1 ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def generate_inference(text):\n",
    "    if isinstance(text, str): text=[text]\n",
    "    text_list = [clean_data(x) for x in text]\n",
    "    token_list = [x.split(\" \") for x in text_list]\n",
    "    encodings = tokenizer.batch_encode_plus(token_list, max_length=max_len_train, padding=True, truncation=True,\n",
    "                                                is_split_into_words=True, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids, attention_mask = encodings.get(\"input_ids\").cuda(), encodings.get(\"attention_mask\").cuda()\n",
    "    output = model.forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    batch_token_ids = input_ids.tolist()\n",
    "    batch_label_ids = output[0].argmax(2).tolist()\n",
    "\n",
    "    tokens = [tokenizer.convert_ids_to_tokens(x) for x in batch_token_ids]\n",
    "    labels = [[id2tag[i] for i in labels] for labels in batch_label_ids]\n",
    "    return tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[CLS]', 'verify', 'for', 'lab', 'batch', 'type', 'selection', ',', 'the', 'following', 'are', 'displayed', ':', '-', 'reason', 'for', 'this', 'batch', '*', ':', 'displays', ':', 'select', 'reason', 'for', 'this', 'batch', '-', 'optional', 'equipment', ':', 'displays', ':', 'select', 'optional', 'equipment', 'capture', 'and', 'attach', 'screen', 'print', ':', 'equipment', 'information', 'section', '[SEP]']]\n",
      "[['O', 'action', 'O', 'data', 'label', 'label', 'label', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'label', 'O', 'label', 'label', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'label', 'O', 'O', 'O', 'O', 'O', 'O', 'action', 'O', 'action', 'label', 'label', 'O', 'O', 'O', 'O', 'O']]\n",
      "[CLS]                ------->                   O\n",
      "verify               ------->              action\n",
      "for                  ------->                   O\n",
      "lab                  ------->                data\n",
      "batch                ------->               label\n",
      "type                 ------->               label\n",
      "selection            ------->               label\n",
      ",                    ------->                   O\n",
      "the                  ------->                   O\n",
      "following            ------->                   O\n",
      "are                  ------->                   O\n",
      "displayed            ------->                   O\n",
      ":                    ------->                   O\n",
      "-                    ------->                   O\n",
      "reason               ------->               label\n",
      "for                  ------->                   O\n",
      "this                 ------->               label\n",
      "batch                ------->               label\n",
      "*                    ------->                   O\n",
      ":                    ------->                   O\n",
      "displays             ------->                   O\n",
      ":                    ------->                   O\n",
      "select               ------->                   O\n",
      "reason               ------->                   O\n",
      "for                  ------->                   O\n",
      "this                 ------->                   O\n",
      "batch                ------->                   O\n",
      "-                    ------->                   O\n",
      "optional             ------->                   O\n",
      "equipment            ------->               label\n",
      ":                    ------->                   O\n",
      "displays             ------->                   O\n",
      ":                    ------->                   O\n",
      "select               ------->                   O\n",
      "optional             ------->                   O\n",
      "equipment            ------->                   O\n",
      "capture              ------->              action\n",
      "and                  ------->                   O\n",
      "attach               ------->              action\n",
      "screen               ------->               label\n",
      "print                ------->               label\n",
      ":                    ------->                   O\n",
      "equipment            ------->                   O\n",
      "information          ------->                   O\n",
      "section              ------->                   O\n",
      "[SEP]                ------->                   O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Verify for Lab Batch Type selection, the following are displayed:\n",
    "- Reason for this Batch*: displays: Select Reason for this Batch\n",
    "- Optional Equipment: displays: Select Optional Equipment\n",
    "\n",
    "Capture and attach screen print: Equipment Information section\"\"\"\n",
    "\n",
    "batch_tokens, batch_labels = generate_inference(text)\n",
    "print(batch_tokens);print(batch_labels)\n",
    "\n",
    "for token, label in zip(batch_tokens, batch_labels):\n",
    "    for t, l in (zip(token, label)):\n",
    "        print(f\"{t:<20} ------->{l:>20}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'action', 1: 'label', 2: 'data', 3: 'O'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify for Lab Batch Type selection, the following are displayed:\n",
      "- Reason for this Batch*: displays: Select Reason for this Batch\n",
      "- Optional Equipment: displays: Select Optional Equipment\n",
      "\n",
      "Capture and attach screen print: Equipment Information section \n",
      "\n",
      "{'action': ['verify', 'capture', 'attach'], 'data': ['lab'], 'label': ['batch type selection', 'reason for this batch', 'optional equipment', 'screen print']} \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def get_results(text):\n",
    "    result = []\n",
    "    if isinstance(text, str): text=[text]\n",
    "    batch_tokens, batch_labels = generate_inference(text)\n",
    "    for index, (tokens, labels) in enumerate(zip(batch_tokens, batch_labels)):\n",
    "        prev_label, token_list, entities = labels[0], [tokens[0]], []\n",
    "        for token_index, (token, label) in enumerate(list(zip(tokens[1:], labels[1:]))):\n",
    "            label = label.split(\"-\")[-1]\n",
    "            if label == 'O':\n",
    "                if prev_label != 'O': entities.append((token_list, prev_label))\n",
    "                token_list, prev_label = list(), label\n",
    "\n",
    "            if prev_label == label:\n",
    "                token_list.append( token)\n",
    "            else:\n",
    "                if prev_label != 'O': entities.append((token_list, prev_label))\n",
    "                token_list = list()\n",
    "                prev_label = label\n",
    "                token_list.append(token)\n",
    "        if token_list:\n",
    "            entities.append((token_list, prev_label))\n",
    "\n",
    "        #print(entities)\n",
    "        entity_dict = {}\n",
    "        _ = [entity_dict.setdefault(x[1], []).append(\" \".join(x[0])) for x in entities if x[0]]\n",
    "        entity_dict.pop(\"O\", None)\n",
    "        result.append(entity_dict)\n",
    "        print(text[index], \"\\n\")\n",
    "        print(entity_dict, \"\\n\")\n",
    "        print(\"=\" * 50)\n",
    "get_results(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the following for the Equipment Information section:\n",
      "a. - Site\n",
      "b. - Legal Product Category \n",
      "\n",
      "{'action': ['select'], 'label': ['site', 'legal product category']} \n",
      "\n",
      "==================================================\n",
      "Click 'Actions' dropdown and select 'Post to Ledger' \n",
      "\n",
      "{'action': ['click', 'select'], 'label': ['actions', 'post to ledger', '[PAD] [PAD]']} \n",
      "\n",
      "==================================================\n",
      "Navigate to Manage Users Page [Overview->Manage Users] and click on Add User button \n",
      "\n",
      "{'action': ['navigate', 'click'], 'label': ['manage users', 'add user']} \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"\"\"Select the following for the Equipment Information section:\n",
    "a. - Site\n",
    "b. - Legal Product Category\"\"\",\n",
    "\"\"\"Click 'Actions' dropdown and select 'Post to Ledger'\"\"\",\n",
    "\"\"\"Navigate to Manage Users Page [Overview->Manage Users] and click on Add User button\"\"\"]\n",
    "\n",
    "get_results(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Click on please accept this invitation \n",
      "\n",
      "{'action': ['click', 'accept'], 'label': ['invitation']} \n",
      "\n",
      "==================================================\n",
      "Navigate to website by clicking on login button \n",
      "\n",
      "{'action': ['navigate', 'clicking'], 'label': ['website', 'log ##in']} \n",
      "\n",
      "==================================================\n",
      "I want to Login to the website \n",
      "\n",
      "{'action': ['log ##in']} \n",
      "\n",
      "==================================================\n",
      "This website has to be login \n",
      "\n",
      "{'action': ['log ##in']} \n",
      "\n",
      "==================================================\n",
      "open the app drawer \n",
      "\n",
      "{'action': ['open'], 'label': ['app drawer', '[PAD]', '[PAD]']} \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"Click on please accept this invitation\",\n",
    "            \"Navigate to website by clicking on login button\",\n",
    "            \"I want to Login to the website\", \n",
    "             \"This website has to be login\", \n",
    "             \"open the app drawer\"]\n",
    "get_results(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
